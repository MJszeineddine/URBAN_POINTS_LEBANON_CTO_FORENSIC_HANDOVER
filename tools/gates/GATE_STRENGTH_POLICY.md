# Gate Strength Policy

## Purpose

This policy defines what constitutes acceptable gate evidence for the Urban Points Lebanon project. It ensures that orders marked "Done" are backed by real execution evidence, not hand-written or forged verdicts.

## Core Principle

**Evidence > Claims**

A gate is only valid if it produces verifiable evidence of actual execution, not just textual assertions.

---

## Acceptable Gate Evidence (Minimum Requirements)

### 1. Gate Command Requirements

Every order MUST have a non-empty, non-trivial `Gate_Command` in the ORDERS sheet:

✅ **Acceptable:**
- `bash tools/gates/ord_01.sh`
- `npm test && npm run build`
- `flutter test integration_test/`
- `python3 tools/gates/verify_backend.py`

❌ **Unacceptable:**
- Empty/missing gate_command
- `echo "PASS"` (echo-only, no execution)
- Trivial commands that produce no meaningful output

---

### 2. Evidence Folder Structure

Each order's latest evidence folder (`docs/evidence/<ORDER_ID>/<RUN_TS>/`) MUST contain:

#### Required Files:
1. **gate.log** - Full stdout/stderr from gate execution
   - Minimum size: **2048 bytes** (2 KB)
   - Must contain at least **2 execution signatures** (see below)
   
2. **verdict.json** - Structured result
   - Must be valid JSON
   - Required fields:
     - `order_id` (matches folder name)
     - `run_ts` (matches folder timestamp format YYYYMMDD-HHMMSS)
     - `end_to_end_working` (boolean, must be `true` for Done orders)
     - `artifacts` (list, must include at least gate.log and verdict.json)
   - Must be created **AFTER** gate.log (mtime check)

---

### 3. Execution Signatures

`gate.log` must contain evidence of actual execution. Acceptable signatures include:

#### Build/Test Frameworks:
- `npm run` - Node.js execution
- `flutter` - Flutter commands
- `firebase` - Firebase CLI
- `jest` - Jest test runner
- `playwright` - Playwright e2e tests
- `gradle` - Gradle build

#### Build/Test Results:
- `BUILD SUCCESS` or `BUILD FAILED`
- `PASS` or `FAIL` (test results)
- `Tests:` (test summary)
- `exit code` or `Exit code` (command exit codes)

**Minimum:** At least **2 different signatures** must appear in gate.log.

---

### 4. Feature Matrix Consistency

If an order lists `Feature_IDs`, every linked feature MUST:
- Exist in `FULL_STACK_FEATURE_MATRIX` sheet
- Have `Is_End_to_End_Working` = `"YES"`

If any feature is not marked "YES", the order CANNOT be marked Done.

---

## Disallowed Patterns

### ❌ Hand-Written Verdicts
```json
// verdict.json created manually without running gate
{
  "order_id": "ORD-01",
  "end_to_end_working": true
}
```
**Why:** No proof of execution. verdict.json must be generated by gate script after running tests/checks.

### ❌ Echo-Only Gates
```bash
# Gate that just prints success
echo "✓ ORD-01 checks passed"
```
**Why:** No actual validation performed. Gate must run real checks (build/test/lint/etc).

### ❌ Empty or Minimal gate.log
```
# gate.log with only 50 bytes
Running ORD-01...
Done.
```
**Why:** Insufficient evidence. Real gates produce substantial logs from build tools, test runners, etc.

### ❌ Verdict Before Log
```
verdict.json mtime: 2026-01-10 10:00:00
gate.log mtime:     2026-01-10 10:00:05
```
**Why:** Indicates verdict was created before gate ran, suggesting forgery.

---

## How to Extend Execution Signatures

If you introduce new build/test tools, update the signature list in:

**File:** `tools/gates/final_evidence_audit.py`

**Constant:** `EXECUTION_SIGNATURES`

**Add patterns like:**
```python
EXECUTION_SIGNATURES = [
    "npm run",
    "flutter",
    # ... existing ...
    "your_new_tool",  # Add here
]
```

**Then update this policy document** to document the new signature.

---

## Audit Enforcement

The **Final Evidence Audit** (`tools/gates/final_evidence_audit.sh`) enforces this policy by:

1. Checking every Done order has evidence
2. Validating gate.log size >= 2048 bytes
3. Counting execution signatures (minimum 2)
4. Verifying verdict.json structure and fields
5. Checking verdict.json created after gate.log
6. Cross-referencing Feature_IDs with matrix

**If any check fails:** Audit returns exit code 1, creates `NO_GO.md`, and the order must be re-run or marked NO-GO.

---

## Examples

### ✅ Good Evidence

**Gate Command:** `bash tools/gates/ord_01.sh`

**gate.log (excerpt):**
```
Running ORD-01 Auth Parity Check...
npm run test:auth
> urban-points-lebanon@1.0.0 test:auth
> jest src/auth --coverage

PASS  src/auth/login.test.ts
PASS  src/auth/otp.test.ts

Tests:       12 passed, 12 total
Coverage:    95.2%

Exit code: 0
```

**verdict.json:**
```json
{
  "order_id": "ORD-01",
  "run_ts": "20260110-120000",
  "end_to_end_working": true,
  "artifacts": [
    "docs/evidence/ORD-01/20260110-120000/gate.log",
    "docs/evidence/ORD-01/20260110-120000/verdict.json"
  ],
  "blockers": []
}
```

**Why Good:**
- gate.log shows real npm test execution
- Multiple signatures: `npm run`, `jest`, `PASS`, `Tests:`, `Exit code`
- gate.log is >2KB
- verdict.json has all required fields
- verdict created after gate.log

---

### ❌ Bad Evidence

**Gate Command:** `echo "ORD-01 complete"`

**gate.log:**
```
ORD-01 complete
```

**verdict.json:**
```json
{
  "end_to_end_working": true
}
```

**Why Bad:**
- gate.log only 20 bytes (< 2048)
- No execution signatures
- verdict.json missing order_id, run_ts, artifacts
- No proof any tests or checks ran

---

## Questions?

If unsure whether your gate meets the policy:

1. Run: `python3 tools/gates/cto_verify.py --output-dir docs/evidence/CTO_VERIFY/<RUN_TS>`
2. Review `verify.md` for detailed analysis
3. Run: `bash tools/gates/final_evidence_audit.sh`
4. If audit fails, read `NO_GO.md` for specific issues

**Bottom line:** If the audit passes, your evidence is sufficient.
